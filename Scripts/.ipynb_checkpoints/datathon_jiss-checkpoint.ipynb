{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conoco Phillips Dataset Challenge - Team J.J.I.S\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The initial portion of our code is import statements and our data cleaning function. We experimented with a variety of ways to process the data, mainly due to the issue of large amounts of 'na's being found within. One of the methods implemented was replacing all entries of 'na' with -1 , we also tried dropping all columns above a certain 'na' threshold. In the end we found that a threshold of 79% resulted in the highest score. This threshold was decided based on sensors 41, 42, and 43 being sequential while also having a similar amount of 'na' percentage, around 80%. \n",
    "\n",
    "We attempted a few methods of normalization within the data, such as standard and robust normalization. They were implemented using the standard scikit library functions StandardScaler and RobustScaler. However, we found that normalizing the data resulted in lower accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import settings\n",
    "import os\n",
    "os.chdir(\"../Data\")\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "# from sklearn import neighbors\n",
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "\n",
    "def clean_data(df_clean):\n",
    "    # Drop columns with greater than 30% of values equal to na\n",
    "    columns = list(df_clean)\n",
    "    for col in columns:\n",
    "        try:\n",
    "            if df_clean[col].str.count('na').sum() > (60001 * .30):\n",
    "                df_clean = df_clean.drop(columns=[col])\n",
    "        except:\n",
    "            pass\n",
    "    # Count na's in row and drop rows with greater than 30%\n",
    "    df_clean['sum_na'] = df_clean.apply(lambda row: sum(row[0:60000]=='na'), axis=1)\n",
    "    df_done = df_clean[~(df_clean['sum_na'] >= (172 * .3))]\n",
    "    df_done = df_done.drop(columns=['sum_na'])\n",
    "    df_done = df_done.replace('na', -1)\n",
    "\n",
    "    return df_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested machine learning algorithms such as SVM, Random Forest and AdaBoost. For the majority of experiments, random forest gave the best performance regardless of the cleaning or normalization method implemented. For random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...\n"
     ]
    }
   ],
   "source": [
    "def gen_output(predictions):\n",
    "    # columns = ['id', 'target']\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    settings.init()\n",
    "    print(\"Executing...\")\n",
    "\n",
    "    df = pd.read_csv(settings.training_data)\n",
    "    X = df.iloc[:, 2:]  # First two columns are id and target\n",
    "    Y = np.array(df.iloc[:, 1])\n",
    "\n",
    "    # Algorithms\n",
    "    # knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "    alg = RandomForestClassifier(n_estimators=100)\n",
    "    # sv = svm.SVC(kernel='linear')\n",
    "    # ada = AdaBoostClassifier(n_estimators=100)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "    fscores = []\n",
    "    for train, test in cv.split(X, Y):\n",
    "        model = alg.fit(X.iloc[train], Y[train])\n",
    "        Y_pred = model.predict(X.iloc[test])\n",
    "        fscore = f1_score(Y[test], Y_pred, average='weighted', labels=np.unique(Y[test]))\n",
    "        fscores.append(fscore)\n",
    "        print(fscore)\n",
    "        \n",
    "    importances = list(model.feature_importances_)\n",
    "    column_headers = list(df.columns.values)\n",
    "    dicy = dict(zip(importances,column_headers))\n",
    "    dicysort = collections.OrderedDict(sorted(dicy.items()))\n",
    "    sns.set(style='whitegrid')\n",
    "    ax = sns.barplot(x=\"day\", y=\"total_bill\", data=dict(dicysort))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print('Average F-measure:', sum(fscores) / len(fscores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
